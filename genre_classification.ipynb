{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDAys5NJ9vU7"
   },
   "source": [
    "## Wavelet data generation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hU730GYrfDj"
   },
   "outputs": [],
   "source": [
    "def func(cls):\n",
    "  img_names = os.listdir('genres/'+cls)\n",
    "  os.makedirs('wavelets/train/'+cls)\n",
    "  os.makedirs('wavelets/test/'+cls)\n",
    "  print(cls)\n",
    "  train_names = img_names[:60]\n",
    "  test_names = img_names[60:]\n",
    "  cnt = 0\n",
    "  for nm in train_names:\n",
    "    cnt+=1\n",
    "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
    "    #plt.figure(figsize=(14, 5))\n",
    "    librosa.display.waveplot(x)\n",
    "    plt.savefig('wavelets/train/'+cls+'/'+str(cnt)+'.png')\n",
    "    plt.close()\n",
    "  \n",
    "  cnt = 0\n",
    "  for nm in test_names:\n",
    "    cnt+=1\n",
    "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
    "    #plt.figure(figsize=(14, 5))\n",
    "    librosa.display.waveplot(x)\n",
    "    plt.savefig('wavelets/test/'+cls+'/'+str(cnt)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G7aFO6Q4KOXi",
    "outputId": "ea634229-3cad-40b1-e6d7-ec97b12ac901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "classes = [a for a in os.listdir('genres') if '.' not in a]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKQS6NweDHi6"
   },
   "source": [
    "## Spectrogram generation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnwuA7h19z48"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "def func1(cls):\n",
    "  img_names = os.listdir('genres/'+cls)\n",
    "  os.makedirs('spectrogram/train/'+cls)\n",
    "  os.makedirs('spectrogram/test/'+cls)\n",
    "  print(cls)\n",
    "  train_names = img_names[:60]\n",
    "  test_names = img_names[60:]\n",
    "  cnt = 0\n",
    "  for nm in train_names:\n",
    "    cnt+=1\n",
    "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
    "    X = librosa.stft(x)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    librosa.display.specshow(Xdb)\n",
    "    plt.savefig('spectrogram/train/'+cls+'/'+str(cnt)+'.png')\n",
    "    plt.close()\n",
    "  \n",
    "  cnt = 0\n",
    "  for nm in test_names:\n",
    "    cnt+=1\n",
    "    x , sr = librosa.load('genres/'+cls+'/'+nm)\n",
    "    X = librosa.stft(x)\n",
    "    Xdb = librosa.amplitude_to_db(abs(X))\n",
    "    librosa.display.specshow(Xdb)\n",
    "    plt.savefig('spectrogram/test/'+cls+'/'+str(cnt)+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrtBWlF4_YXb",
    "outputId": "c8732337-479b-4e71-fa07-4132d24cefdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "classes = [a for a in os.listdir('genres') if '.' not in a]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQy_bqnn9rmj"
   },
   "source": [
    "## Model training.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgrqLPZcK0MX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjWqYansStBI"
   },
   "outputs": [],
   "source": [
    "labels = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "img_size = 256\n",
    "def get_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZev-1aYS5tp"
   },
   "outputs": [],
   "source": [
    "train = get_data('spectrogram/train')\n",
    "val = get_data('spectrogram/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aOMMXUMnTNAt"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train:\n",
    "  x_train.append(feature)\n",
    "  y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "  x_val.append(feature)\n",
    "  y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5HMhdmkUTebr"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        #horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frApKmq_Tnkk"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(256,256,3)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJFFmPkVTr-f"
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RciGH_6YTzlf"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train,y_train,epochs = 500, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYRkEJTjmAiF"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "model.save_weights('500_epoch_simple_lr.cpkt')\n",
    "\n",
    "pickle.dump(history.history, open('history_500_epoch_simple.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GJSYHmQT6EA"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "rSSZy4h5XwKg",
    "outputId": "4e8c12b5-5fb2-4d58-e512-a7ae663916d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x432 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "history = pickle.load(open('history_500_epoch_simple.pkl','rb'))\n",
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
    "ax1.legend()\n",
    "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
    "ax1.set_ylabel('Accuracy',fontsize=18)\n",
    "ax1.set_xlabel('Epoch',fontsize=18)\n",
    "\n",
    "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
    "ax2.legend()\n",
    "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
    "ax2.set_ylabel('Loss',fontsize=18)\n",
    "ax2.set_xlabel('Epoch',fontsize=18)\n",
    "fig.tight_layout(pad=3.0)\n",
    "#plt.show()\n",
    "plt.savefig('sim_plot1.png',bbox_inches = 'tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLe4sDlebDiE"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_val, predictions, target_names = labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbG03a-7Yayv"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "cm1 = confusion_matrix(y_val, predictions)\n",
    "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
    "              columns = [i for i in labels])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
    "plt.savefig('confusion_mrtx1.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhAssCnpc-zC"
   },
   "source": [
    "# Transfer Learning based modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL_xNwxtbZGE"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape = (256, 256, 3), include_top = False, weights = \"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMP5zEkjboya"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "model = tf.keras.Sequential([base_model,\n",
    "                                 tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                 tf.keras.layers.Dropout(0.2),\n",
    "                                 tf.keras.layers.Dense(10, activation=\"softmax\")                                     \n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skCTJ9t8bzN5"
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history1 = model.fit(x_train,y_train,epochs = 500 , validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvaP0P-cD-Hv"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D30aRr_Lfc9M"
   },
   "outputs": [],
   "source": [
    "acc = history1.history['accuracy']\n",
    "val_acc = history1.history['val_accuracy']\n",
    "loss = history1.history['loss']\n",
    "val_loss = history1.history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-opt3HYbhRP"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "history = pickle.load(open('history_500_epoch_tr.pkl','rb'))\n",
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
    "ax1.legend()\n",
    "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
    "ax1.set_ylabel('Accuracy',fontsize=18)\n",
    "ax1.set_xlabel('Epoch',fontsize=18)\n",
    "\n",
    "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
    "ax2.legend()\n",
    "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
    "ax2.set_ylabel('Loss',fontsize=18)\n",
    "ax2.set_xlabel('Epoch',fontsize=18)\n",
    "fig.tight_layout(pad=3.0)\n",
    "#plt.show()\n",
    "plt.savefig('tfr_plot1.png',bbox_inches = 'tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uf1JjtBAfrSi"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "print(classification_report(y_val, predictions, target_names = labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1UcfoXZcE7z"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "cm1 = confusion_matrix(y_val, predictions)\n",
    "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
    "              columns = [i for i in labels])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
    "plt.savefig('confusion_mrtx2.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RwzSfGdsshTN"
   },
   "outputs": [],
   "source": [
    "model.save_weights('500_epoch_transfer_lr.cpkt')\n",
    "pickle.dump(history1.history, open('history_500_epoch_tr.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkSwbJkkV5cp"
   },
   "source": [
    "## Multi-Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "sRHHpGBdV8fQ"
   },
   "outputs": [],
   "source": [
    "sp_train = get_data('spectrogram/train')\n",
    "sp_val = get_data('spectrogram/test')\n",
    "\n",
    "wv_train = get_data('wavelets/train')\n",
    "wv_val = get_data('wavelets/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh0O-VhDWHiX"
   },
   "outputs": [],
   "source": [
    "x_sp_train = []\n",
    "y_sp_train = []\n",
    "x_sp_val = []\n",
    "y_sp_val = []\n",
    "\n",
    "for feature, label in sp_train:\n",
    "  x_sp_train.append(feature)\n",
    "  y_sp_train.append(label)\n",
    "\n",
    "for feature, label in sp_val:\n",
    "  x_sp_val.append(feature)\n",
    "  y_sp_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_sp_train = np.array(x_sp_train) / 255\n",
    "x_sp_val = np.array(x_sp_val) / 255\n",
    "\n",
    "x_sp_train.reshape(-1, img_size, img_size, 1)\n",
    "y_sp_train = np.array(y_sp_train)\n",
    "\n",
    "x_sp_val.reshape(-1, img_size, img_size, 1)\n",
    "y_sp_val = np.array(y_sp_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWFkA_iJW7v_"
   },
   "outputs": [],
   "source": [
    "x_wv_train = []\n",
    "y_wv_train = []\n",
    "x_wv_val = []\n",
    "y_wv_val = []\n",
    "\n",
    "for feature, label in wv_train:\n",
    "  x_wv_train.append(feature)\n",
    "  y_wv_train.append(label)\n",
    "\n",
    "for feature, label in wv_val:\n",
    "  x_wv_val.append(feature)\n",
    "  y_wv_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_wv_train = np.array(x_wv_train) / 255\n",
    "x_wv_val = np.array(x_wv_val) / 255\n",
    "\n",
    "x_wv_train.reshape(-1, img_size, img_size, 1)\n",
    "y_wv_train = np.array(y_wv_train)\n",
    "\n",
    "x_wv_val.reshape(-1, img_size, img_size, 1)\n",
    "y_wv_val = np.array(y_wv_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQoJT8abXgqm"
   },
   "outputs": [],
   "source": [
    "datagen_sp = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        #horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen_sp.fit(x_sp_train)\n",
    "\n",
    "datagen_wv = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        #rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        #horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen_wv.fit(x_wv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAk1TubMZjJV"
   },
   "outputs": [],
   "source": [
    "input_sp = keras.Input(shape=(256,256,3))\n",
    "input_wv = keras.Input(shape=(256,256,3))\n",
    "\n",
    "x = Conv2D(32,3,padding=\"same\", activation=\"relu\")(input_sp)\n",
    "x = MaxPool2D()(x)\n",
    "x = Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "x = MaxPool2D()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128,activation=\"relu\")(x)\n",
    "x = keras.Model(inputs=input_sp, outputs=x)\n",
    "\n",
    "y = Conv2D(32,3,padding=\"same\", activation=\"relu\")(input_wv)\n",
    "y = MaxPool2D()(y)\n",
    "y = Conv2D(64, 3, padding=\"same\", activation=\"relu\")(y)\n",
    "y = MaxPool2D()(y)\n",
    "y = Dropout(0.4)(y)\n",
    "y = Flatten()(y)\n",
    "y = Dense(128,activation=\"relu\")(y)\n",
    "y = keras.Model(inputs=input_wv, outputs=y)\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "combined = concatenate([x.output, y.output])\n",
    "\n",
    "z = Dense(32, activation=\"relu\")(combined)\n",
    "z = Dense(10, activation=\"softmax\")(z)\n",
    "\n",
    "model = keras.Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K25O827PcoKy"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LsfjV2ucprS"
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr=0.0001)\n",
    "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cPYQFBDc1KR"
   },
   "outputs": [],
   "source": [
    "history2 = model.fit([x_sp_train,x_wv_train],y_sp_train,epochs = 500, validation_data = ([x_sp_val,x_wv_val], y_sp_val))\n",
    "import pickle\n",
    "model.save_weights('500_epoch_multi_lr.cpkt')\n",
    "pickle.dump(history2.history, open('history_500_epoch_multi.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_5tiBlgtduBp"
   },
   "outputs": [],
   "source": [
    "acc = history2.history['accuracy']\n",
    "val_acc = history2.history['val_accuracy']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "\n",
    "plt.figure(figsize=(25, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMQeqjWgeIlw"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "history = pickle.load(open('history_500_epoch_multi.pkl','rb'))\n",
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "epochs_range = range(500)\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.plot(epochs_range, acc, label='Training Accuracy', c = '#4CAF50', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc, label='Validation Accuracy', c='red', linewidth=4)\n",
    "ax1.legend()\n",
    "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
    "ax1.set_ylabel('Accuracy',fontsize=18)\n",
    "ax1.set_xlabel('Epoch',fontsize=18)\n",
    "\n",
    "ax2.plot(epochs_range, loss, label='Training Loss',c = '#4CAF50', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss, label='Validation Loss', c='red', linewidth=4)\n",
    "ax2.legend()\n",
    "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
    "ax2.set_ylabel('Loss',fontsize=18)\n",
    "ax2.set_xlabel('Epoch',fontsize=18)\n",
    "fig.tight_layout(pad=3.0)\n",
    "#plt.show()\n",
    "plt.savefig('multi_plot1.png',bbox_inches = 'tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZP-YbdgId-lQ"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict([x_sp_val,x_wv_val])\n",
    "print(classification_report(y_wv_val, np.argmax(predictions, axis=1), target_names = labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VyAzWhIemG_"
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "cm1 = confusion_matrix(y_val, predictions)\n",
    "df_cm = pd.DataFrame(cm1, index = [i for i in labels],\n",
    "              columns = [i for i in labels])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
    "plt.savefig('confusion_mrtx3.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4uBCt0dd36HJ",
    "outputId": "0ed11f0d-fa2a-4c14-b5c1-123ebf734a64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x432 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_range = range(500)\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "history = pickle.load(open('history_500_epoch_simple.pkl','rb'))\n",
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "\n",
    "history = pickle.load(open('history_500_epoch_tr.pkl','rb'))\n",
    "acc1 = history['accuracy']\n",
    "val_acc1 = history['val_accuracy']\n",
    "loss1 = history['loss']\n",
    "val_loss1 = history['val_loss']\n",
    "\n",
    "history = pickle.load(open('history_500_epoch_multi.pkl','rb'))\n",
    "acc2 = history['accuracy']\n",
    "val_acc2 = history['val_accuracy']\n",
    "loss2 = history['loss']\n",
    "val_loss2 = history['val_loss']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=10)\n",
    "ax1.plot(epochs_range, acc, label='CNN Training Accuracy', c = '#4CAF50', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc, label='CNN Validation Accuracy', c='red', linewidth=4)\n",
    "ax1.plot(epochs_range, acc1, label='Transfer learning Training Accuracy', c = '#e72866', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc1, label='Transfer learning Validation Accuracy', c='#282ec7', linewidth=4)\n",
    "ax1.plot(epochs_range, acc2, label='Multi modal Training Accuracy', c = '#171c1c', linewidth=4)\n",
    "ax1.plot(epochs_range, val_acc2, label='Multi modal Validation Accuracy', c='#62176e', linewidth=4)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
    "ax1.set_ylabel('Accuracy',fontsize=18)\n",
    "ax1.set_xlabel('Epoch',fontsize=18)\n",
    "\n",
    "ax2.plot(epochs_range, loss, label='CNN Training Loss',c = '#4CAF50', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss, label='CNN Validation Loss', c='red', linewidth=4)\n",
    "ax2.plot(epochs_range, loss1, label='Transfer learning Training Loss',c = '#c72866', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss1, label='Transfer learning Validation Loss', c='#282ec7', linewidth=4)\n",
    "ax2.plot(epochs_range, loss2, label='Multi modal Training Loss',c = '#171c1c', linewidth=4)\n",
    "ax2.plot(epochs_range, val_loss2, label='Multi modal learning Validation Loss', c='#62176e', linewidth=4)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
    "ax2.set_ylabel('Loss',fontsize=18)\n",
    "ax2.set_xlabel('Epoch',fontsize=18)\n",
    "fig.tight_layout(pad=3.0)\n",
    "#plt.show()\n",
    "plt.savefig('all_1.png',bbox_inches = 'tight')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvqSlPNI4C5b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "genre_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
